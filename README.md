# ğŸ§  AI-Powered Document Search and Question Answering using RAG

A complete Retrieval-Augmented Generation (RAG) system that allows users to upload documents and ask natural language questions, with answers generated from the uploaded document content.

## ğŸš€ Features

- **Document Ingestion**: Upload PDFs, TXT files, DOCX files, or fetch from URLs
- **Text Extraction**: Automatic text extraction from various document formats
- **Intelligent Chunking**: Split documents into overlapping segments for better retrieval
- **Vector Embeddings**: Generate embeddings using sentence-transformers or OpenAI
- **Vector Store**: FAISS-based local vector database for fast similarity search
- **RAG Pipeline**: Retrieve relevant chunks and generate context-grounded answers
- **Web Interface**: User-friendly Streamlit interface
- **Conversation History**: Save and view Q&A history
- **Source Citation**: See which parts of documents were used for answers

## ğŸ“‹ Prerequisites

### For Docker:
- Docker and Docker Compose installed
- OpenAI API key (**required** for LLM question answering)

### For Local Installation:
- Python 3.8+
- Google Gemini api key (**required** for LLM question answering)

> **ğŸ’¡ Need an API key?** Get one free at https://platform.openai.com/api-keys (you get free credits to start!)

## ğŸ› ï¸ Installation

### Option 1: Docker (Recommended)

1. Clone or download this repository

2. Build and run with Docker Compose:
```bash
# Set your OpenAI API key (optional)
export OPENAI_API_KEY="your-api-key-here"

# Build and run
docker-compose up --build
```

Or using Docker directly:
```bash
# Build the image
docker build -t ai-doc-search-rag .

# Run the container
docker run -p 8501:8501 \
  -v $(pwd)/models:/app/models \
  -e OPENAI_API_KEY="your-api-key-here" \
  ai-doc-search-rag
```

The application will be available at `http://localhost:8501`

### Option 2: Local Installation

1. Clone or download this repository

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set OpenAI API key (required for LLM):
   
   **Option A: Environment Variable (Recommended)**
   ```bash
   # Windows PowerShell
   $env:OPENAI_API_KEY="your-api-key-here"
   
   # Windows CMD
   set OPENAI_API_KEY=your-api-key-here
   
   # Linux/Mac
   export OPENAI_API_KEY="your-api-key-here"
   ```
   
   **Option B: .env File**
   ```bash
   cp .env.example .env
   # Edit .env and add your API key
   ```
   
   **Option C: Streamlit UI**
   - Enter it in the sidebar when the app runs
   
   ğŸ“– **See [SETUP_API_KEY.md](SETUP_API_KEY.md) for detailed instructions**

## ğŸ¯ Usage

### Docker
```bash
docker-compose up
```
Access at `http://localhost:8501`

### Local
1. Start the Streamlit application:
```bash
streamlit run app.py
```

2. **Configure the system** (in the sidebar):
   - Choose whether to use OpenAI embeddings (default: sentence-transformers)
   - Choose LLM model (default: GPT-3.5-turbo)
   - Enter OpenAI API key if using OpenAI services
   - Click "Initialize System"

3. **Upload documents**:
   - Go to "Upload Documents" tab
   - Upload PDF/TXT/DOCX files or enter a URL
   - Click "Process Document"
   - Wait for indexing to complete

4. **Ask questions**:
   - Go to "Ask Questions" tab
   - Enter your question
   - Adjust number of chunks to retrieve (default: 5)
   - Click "Get Answer"
   - View answer with source citations

5. **View history**:
   - Check "History" tab for past Q&A sessions

## ğŸ“ Project Structure

```
ai_doc_search_rag/
â”‚
â”œâ”€â”€ app.py                 # Main Streamlit web interface
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ Dockerfile            # Docker container definition
â”œâ”€â”€ docker-compose.yml    # Docker Compose configuration
â”œâ”€â”€ docker-run.sh         # Convenience script (Linux/Mac)
â”œâ”€â”€ docker-run.bat        # Convenience script (Windows)
â”œâ”€â”€ .dockerignore         # Files to exclude from Docker build
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ extract_text.py   # PDF/DOCX/URL text extraction
â”‚   â”œâ”€â”€ chunker.py        # Text chunking logic
â”‚   â””â”€â”€ embed_store.py    # Embedding + vector store setup
â”‚
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ rag_pipeline.py   # RAG: retrieve + generate answer
â”‚
â””â”€â”€ models/
    â”œâ”€â”€ vector_store/     # FAISS index and metadata
    â””â”€â”€ chunks_storage.pkl # Stored chunk texts
```

## ğŸ”§ Architecture

### Document Processing Flow

1. **Extraction**: Document â†’ Text (using PyMuPDF, python-docx, or BeautifulSoup)
2. **Chunking**: Text â†’ Overlapping chunks (500 tokens, 50 overlap)
3. **Embedding**: Chunks â†’ Vector embeddings (sentence-transformers or OpenAI)
4. **Storage**: Embeddings â†’ FAISS vector database

### Question Answering Flow

1. **Query Embedding**: Question â†’ Vector embedding
2. **Retrieval**: Search FAISS for top-k similar chunks
3. **Context Building**: Combine retrieved chunks as context
4. **Generation**: LLM generates answer from context + question
5. **Response**: Answer + source citations

## âš™ï¸ Configuration Options

### Embedding Models
- **Default**: `sentence-transformers/all-MiniLM-L6-v2` (free, local)
- **OpenAI**: `text-embedding-ada-002` (requires API key)

### LLM Models
- `gpt-3.5-turbo` (default, cost-effective)
- `gpt-4` (higher quality, more expensive)
- `gpt-4-turbo-preview` (latest GPT-4 variant)

### Chunking Parameters
- Chunk size: 500 tokens
- Overlap: 50 tokens
- Adjustable in code

## ğŸ¨ Features in Detail

### Document Formats Supported
- **PDF**: Extracts text from all pages
- **TXT**: Plain text files (UTF-8 or Latin-1)
- **DOCX**: Microsoft Word documents
- **URLs**: HTML pages or PDFs from URLs

### Vector Search
- Uses FAISS (Facebook AI Similarity Search) for fast similarity search
- L2 distance metric for finding closest embeddings
- Stores metadata (filename, chunk index, source) with each vector

### RAG Pipeline
- Retrieves top-k most relevant document chunks
- Builds context prompt with retrieved content
- Generates answers that are grounded in document content
- Provides source attribution for transparency

## ğŸ” Example Use Cases

- **Research Paper Q&A**: Upload research papers and ask specific questions
- **Documentation Search**: Index documentation and find answers quickly
- **Legal Document Analysis**: Extract information from contracts or legal docs
- **Educational Content**: Upload textbooks and get study answers
- **Corporate Knowledge Base**: Index internal documents for employee queries

## ğŸ› Troubleshooting

### Docker Issues
- **Port already in use**: Change the port mapping in `docker-compose.yml` (e.g., `"8502:8501"`)
- **Permission errors**: Ensure Docker has access to the `models` directory
- **Build fails**: Make sure you're in the project root directory when building
- **Memory issues**: Increase Docker memory allocation if processing large documents

### Import Errors
- Make sure all dependencies are installed: `pip install -r requirements.txt`
- For FAISS on Mac M1/M2: Use `faiss-cpu` (already in requirements)
- In Docker: All dependencies are pre-installed in the image

### OpenAI API Issues
- Verify API key is correct
- Check API quota/billing
- Try using sentence-transformers instead (free alternative)
- In Docker: Set `OPENAI_API_KEY` as environment variable or in docker-compose.yml

### Document Processing Errors
- Ensure PDFs are not corrupted or password-protected
- Check URL accessibility for web scraping
- Verify file formats are supported
- In Docker: Use volume mounts to access local files (see docker-compose.yml)

## ğŸ“ License

This project is open source and available for educational and commercial use.

## ğŸ¤ Contributing

Feel free to submit issues, fork the repository, and create pull requests.

## ğŸ™ Acknowledgments

- Built with Streamlit, FAISS, sentence-transformers, and OpenAI
- Inspired by RAG architecture from Retrieval-Augmented Generation papers

---

**Enjoy your AI-powered document search system! ğŸš€**

